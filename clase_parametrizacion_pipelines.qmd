---
title: "ParametrizaciÃ³n, Estructura de Carpetas y Pipelines"
subtitle: "De cÃ³digo enredado a pipelines profesionales"
author: "Claude guiado y corregido por Rodo"
format:
  html:
    theme: darkly
    toc: true
    toc-depth: 3
    code-fold: false
    code-tools: true
    highlight-style: monokai
    self-contained: true
---

# 1. IntroducciÃ³n y MotivaciÃ³n

## Â¿Por quÃ© parametrizar?

Cuando trabajamos en proyectos de datos, es comÃºn empezar con cÃ³digo "rÃ¡pido y sucio" para probar ideas. Sin embargo, este cÃ³digo presenta problemas:

- **Valores hardcodeados** dispersos por todo el script
- **DifÃ­cil mantenimiento**: cambiar un parÃ¡metro requiere editar mÃºltiples lÃ­neas
- **No reutilizable**: cada nuevo caso requiere copiar y modificar el cÃ³digo
- **Propenso a errores**: fÃ¡cil olvidar actualizar algÃºn valor
- **DifÃ­cil de versionar**: credenciales y configuraciones mezcladas con lÃ³gica

**Objetivo de esta clase**: Transformar cÃ³digo desorganizado en pipelines profesionales, mantenibles y escalables.

---

# 2. ParametrizaciÃ³n de Variables

## 2.1 De CÃ³digo Enredado a CÃ³digo Limpio

Veamos un ejemplo tÃ­pico de cÃ³digo problemÃ¡tico:

```{python}
#| eval: false
#| code-line-numbers: true

# âŒ CÃ“DIGO MALO: valores hardcodeados y repetidos
import pandas as pd

# Cargar datos con ruta hardcodeada
df = pd.read_csv('/home/usuario/proyectos/datos/ventas_2024.csv')

# Filtrar por fecha (valor repetido)
df_filtrado = df[df['fecha'] >= '2024-01-01']
df_filtrado = df_filtrado[df_filtrado['fecha'] <= '2024-12-31']

# Calcular mÃ©tricas con umbral hardcodeado
clientes_importantes = df_filtrado[df_filtrado['total'] > 1000]

# Guardar en ruta hardcodeada
clientes_importantes.to_csv('/home/usuario/proyectos/resultados/clientes_2024.csv')

print(f"Procesados {len(df)} registros de 2024")
print(f"Encontrados {len(clientes_importantes)} clientes con compras > 1000")
```

**Problemas identificados:**

1. Rutas absolutas que solo funcionan en una mÃ¡quina

2. Fechas repetidas (Â¿quÃ© pasa si queremos analizar 2025?)

3. Umbral mÃ¡gico (1000) sin contexto

4. AÃ±o repetido en strings

Ahora refactoricemos paso a paso:

```{python}
#| eval: false

# âœ… CÃ“DIGO LIMPIO: parametrizado y mantenible
import pandas as pd
from pathlib import Path

# Definir parÃ¡metros en un solo lugar
YEAR = 2024
UMBRAL_CLIENTE_IMPORTANTE = 1000
RUTA_DATOS = Path('data')
RUTA_RESULTADOS = Path('results')

# Crear rutas si no existen
RUTA_RESULTADOS.mkdir(exist_ok=True)

# Nombres de archivos parametrizados
archivo_entrada = RUTA_DATOS / f'ventas_{YEAR}.csv'
archivo_salida = RUTA_RESULTADOS / f'clientes_{YEAR}.csv'

# Cargar datos
df = pd.read_csv(archivo_entrada)

# Filtrar por fecha usando el parÃ¡metro
fecha_inicio = f'{YEAR}-01-01'
fecha_fin = f'{YEAR}-12-31'
df_filtrado = df[
    (df['fecha'] >= fecha_inicio) & 
    (df['fecha'] <= fecha_fin)
]

# Calcular mÃ©tricas usando parÃ¡metro descriptivo
clientes_importantes = df_filtrado[
    df_filtrado['total'] > UMBRAL_CLIENTE_IMPORTANTE
]

# Guardar resultados
clientes_importantes.to_csv(archivo_salida, index=False)

# Reportar resultados
print(f"Procesados {len(df)} registros de {YEAR}")
print(f"Encontrados {len(clientes_importantes)} clientes con compras > ${UMBRAL_CLIENTE_IMPORTANTE}")
```

**Mejoras logradas:**

- âœ… ParÃ¡metros definidos una sola vez

- âœ… Nombres descriptivos en MAYÃšSCULAS para constantes

- âœ… Rutas relativas usando `pathlib`. No es necesario usar `pathlib`, es una opciÃ³n. Pero lo realmente importante es no hardcodear rutas absolutas, sino que usar rutas relativas al proyecto (no al archivo)

- âœ… FÃ¡cil modificar para otro aÃ±o o umbral

---

## 2.2 Variables de Ambiente con .env

Para informaciÃ³n sensible (credenciales, API keys) o configuraciones que cambian entre entornos (desarrollo, producciÃ³n), usamos archivos `.env`.

### InstalaciÃ³n

```{bash}
#| eval: false
pip install python-dotenv
```

### Crear archivo .env

Crear archivo `.env` en la raÃ­z del proyecto:

```{bash}
#| eval: false
#| filename: ".env"

# ConfiguraciÃ³n de base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=analytics_db
DB_USER=analyst
DB_PASSWORD=mi_password_secreto

# ConfiguraciÃ³n de APIs
API_KEY=abc123xyz789
API_ENDPOINT=https://api.ejemplo.com

# ConfiguraciÃ³n del proyecto
ENVIRONMENT=development
LOG_LEVEL=INFO
```

### Uso en Python

```{python}
#| eval: false

import os
from dotenv import load_dotenv

# Cargar variables de ambiente
load_dotenv()

# Acceder a las variables
db_host = os.getenv('DB_HOST')
db_port = os.getenv('DB_PORT')
db_name = os.getenv('DB_NAME')
api_key = os.getenv('API_KEY')

# Valores por defecto si no existe la variable
log_level = os.getenv('LOG_LEVEL', 'WARNING')

print(f"Conectando a {db_host}:{db_port}/{db_name}")
print(f"Nivel de log: {log_level}")
```

### ğŸ”’ Buenas PrÃ¡cticas de Seguridad

1. **NUNCA** subir `.env` a Git:

```{bash}
#| eval: false
#| filename: ".gitignore"

# Ignorar archivos de ambiente
.env
.env.local
.env.*.local
```

2. Crear `.env.example` como plantilla:

```{bash}
#| eval: false
#| filename: ".env.example"

# ConfiguraciÃ³n de base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=nombre_base_datos
DB_USER=tu_usuario
DB_PASSWORD=tu_password

# ConfiguraciÃ³n de APIs
API_KEY=tu_api_key_aqui
```

3. Documentar en README cÃ³mo configurar el `.env`


4. A veces es bueno tener mÃºltiples archivos `.env` para diferentes entornos:

- `.env.development`
- `.env.production`
- `.env.testing`
- `.env`
- etc.

Por defecto `python-dotenv` carga `.env`, pero puedes especificar otro archivo si es necesario. 

QuizÃ¡s algunos de ellos sÃ­ pueden comitearse, y otros no. Por defecto, los archivos `.env` no deberÃ­an comitearse nunca.

---

## 2.3 Argumentos de LÃ­nea de Comandos con argparse

`argparse` permite ejecutar scripts con diferentes parÃ¡metros sin modificar el cÃ³digo.

### Ejemplo BÃ¡sico

Crear archivo `procesar_ventas.py`:

```{python}
#| eval: false
#| filename: "scripts/procesar_ventas.py"

import argparse
import pandas as pd
from pathlib import Path

def procesar_ventas(year, umbral, verbose=False):
    """Procesa ventas de un aÃ±o especÃ­fico."""
    
    # Rutas
    archivo_entrada = Path('data') / f'ventas_{year}.csv'
    archivo_salida = Path('results') / f'clientes_{year}.csv'
    
    if verbose:
        print(f"ğŸ“‚ Leyendo {archivo_entrada}...")
    
    # AquÃ­ irÃ­a la lÃ³gica real de procesamiento
    # Para el ejemplo, creamos datos dummy
    df = pd.DataFrame({
        'cliente_id': range(1, 101),
        'total': [500 + i * 15 for i in range(100)]
    })
    
    clientes_importantes = df[df['total'] > umbral]
    
    # Guardar
    Path('results').mkdir(exist_ok=True)
    clientes_importantes.to_csv(archivo_salida, index=False)
    
    if verbose:
        print(f"âœ… Procesados {len(df)} registros")
        print(f"âœ… {len(clientes_importantes)} clientes > ${umbral}")
        print(f"ğŸ’¾ Guardado en {archivo_salida}")
    
    return len(clientes_importantes)

def main():
    # Configurar parser
    parser = argparse.ArgumentParser(
        description='Procesa ventas y filtra clientes importantes',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Argumentos requeridos
    parser.add_argument(
        '--year',
        type=int,
        required=True,
        help='AÃ±o a procesar (ej: 2024)'
    )
    
    # Argumentos opcionales con valores por defecto
    parser.add_argument(
        '--umbral',
        type=float,
        default=1000,
        help='Umbral mÃ­nimo para cliente importante'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Mostrar mensajes detallados'
    )
    
    # Parsear argumentos
    args = parser.parse_args()
    
    # Ejecutar procesamiento
    n_clientes = procesar_ventas(
        year=args.year,
        umbral=args.umbral,
        verbose=args.verbose
    )
    
    print(f"Proceso completado: {n_clientes} clientes importantes")

if __name__ == '__main__':
    main()
```

### Uso desde Terminal

```{bash}
#| eval: false

# Ver ayuda
python scripts/procesar_ventas.py --help

# EjecuciÃ³n bÃ¡sica (solo argumento requerido)
python scripts/procesar_ventas.py --year 2024

# Con todos los parÃ¡metros
python scripts/procesar_ventas.py --year 2024 --umbral 1500 --verbose

# Forma corta para verbose
python scripts/procesar_ventas.py --year 2025 -v
```

### Tipos de Argumentos Comunes

```{python}
#| eval: false

# String
parser.add_argument('--nombre', type=str, required=True)

# Integer
parser.add_argument('--num-registros', type=int, default=100)

# Float
parser.add_argument('--tasa', type=float, default=0.05)

# Boolean flag
parser.add_argument('--debug', action='store_true')

# Lista de valores
parser.add_argument('--columnas', nargs='+', help='Lista de columnas')

# Opciones limitadas
parser.add_argument(
    '--formato', 
    choices=['csv', 'parquet', 'excel'],
    default='csv'
)
```

---

# 3. ParametrizaciÃ³n de Queries

## 3.1 SQL con f-strings

Al construir queries dinÃ¡micamente, necesitamos insertar parÃ¡metros. Los f-strings son Ãºtiles pero debemos usarlos con cuidado.

### Ejemplo Seguro

```{python}
#| eval: false

import pandas as pd
import sqlite3

# Crear base de datos de ejemplo
conn = sqlite3.connect(':memory:')

# Crear tabla de ejemplo
ventas_data = pd.DataFrame({
    'fecha': pd.date_range('2024-01-01', periods=100),
    'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Laptop'],
    'cantidad': range(1, 101),
    'precio': [1000, 25, 75] * 33 + [1000]
})
ventas_data.to_sql('ventas', conn, index=False, if_exists='replace')

# ParÃ¡metros
fecha_inicio = '2024-01-01'
fecha_fin = '2024-03-31'
producto_filtro = 'Laptop'
cantidad_minima = 50

# Query parametrizada con f-strings
query = f"""
SELECT 
    fecha,
    producto,
    cantidad,
    precio,
    cantidad * precio as total
FROM ventas
WHERE fecha BETWEEN '{fecha_inicio}' AND '{fecha_fin}'
    AND producto = '{producto_filtro}'
    AND cantidad >= {cantidad_minima}
ORDER BY fecha DESC
"""

resultado = pd.read_sql(query, conn)
print(resultado.head())

conn.close()
```

### âš ï¸ PrevenciÃ³n de SQL Injection

**NUNCA** usar f-strings con input de usuarios sin validaciÃ³n:

```{python}
#| eval: false

# âŒ PELIGROSO: vulnerable a SQL injection
producto_usuario = input("Â¿QuÃ© producto buscar? ")
query = f"SELECT * FROM ventas WHERE producto = '{producto_usuario}'"

# âœ… SEGURO: usar parÃ¡metros con placeholder
query = "SELECT * FROM ventas WHERE producto = ?"
resultado = pd.read_sql(query, conn, params=(producto_usuario,))
```

### ConstrucciÃ³n DinÃ¡mica de Queries

```{python}
#| eval: false

def construir_query_ventas(
    columnas=None,
    fecha_inicio=None,
    fecha_fin=None,
    productos=None,
    limite=None
):
    """Construye query dinÃ¡micamente segÃºn parÃ¡metros."""
    
    # Columnas por defecto
    if columnas is None:
        columnas = ['fecha', 'producto', 'cantidad', 'precio']
    
    select_clause = ', '.join(columnas)
    
    # Construir WHERE dinÃ¡micamente
    condiciones = []
    
    if fecha_inicio:
        condiciones.append(f"fecha >= '{fecha_inicio}'")
    
    if fecha_fin:
        condiciones.append(f"fecha <= '{fecha_fin}'")
    
    if productos:
        productos_str = "', '".join(productos)
        condiciones.append(f"producto IN ('{productos_str}')")
    
    where_clause = ' AND '.join(condiciones) if condiciones else '1=1'
    
    # Construir query completa
    query = f"""
    SELECT {select_clause}
    FROM ventas
    WHERE {where_clause}
    """
    
    if limite:
        query += f"\nLIMIT {limite}"
    
    return query

# Ejemplo de uso
query1 = construir_query_ventas(
    fecha_inicio='2024-01-01',
    productos=['Laptop', 'Mouse'],
    limite=10
)
print(query1)
```

---

## 3.2 ParametrizaciÃ³n con Polars (Opcional)

Polars es una alternativa moderna y rÃ¡pida a pandas. Ofrece formas elegantes de parametrizar queries.

```{python}
#| eval: false

import polars as pl

# Crear DataFrame de ejemplo
df = pl.DataFrame({
    'fecha': pl.date_range(
        start=pl.date(2024, 1, 1),
        end=pl.date(2024, 12, 31),
        interval='1d'
    ),
    'producto': ['Laptop', 'Mouse', 'Teclado'] * 122,
    'cantidad': range(1, 367),
    'precio': [1000, 25, 75] * 122
})

# ParÃ¡metros
FECHA_INICIO = pl.date(2024, 6, 1)
FECHA_FIN = pl.date(2024, 6, 30)
PRODUCTOS_INTERES = ['Laptop', 'Mouse']
CANTIDAD_MIN = 100

# Query parametrizada con Polars
resultado = (
    df
    .filter(
        (pl.col('fecha') >= FECHA_INICIO) &
        (pl.col('fecha') <= FECHA_FIN) &
        (pl.col('producto').is_in(PRODUCTOS_INTERES)) &
        (pl.col('cantidad') >= CANTIDAD_MIN)
    )
    .with_columns(
        (pl.col('cantidad') * pl.col('precio')).alias('total')
    )
    .sort('fecha', descending=True)
)

print(resultado)
```

**Ventajas de Polars:**

- MÃ¡s rÃ¡pido que pandas en datasets grandes

- Sintaxis expresiva y type-safe

- Lazy evaluation para optimizaciÃ³n automÃ¡tica

---

# 4. Estructura de Carpetas

## 4.1 Principios de OrganizaciÃ³n

Una buena estructura de carpetas:

- **Facilita encontrar archivos**: ubicaciÃ³n predecible
- **Separa concerns**: datos, cÃ³digo, resultados, documentaciÃ³n
- **Escalable**: crece con el proyecto sin volverse caÃ³tica
- **Reproducible**: otros pueden entender y ejecutar el proyecto

## 4.2 Estructura Recomendada

```
proyecto_analytics/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Datos originales (NUNCA modificar)
â”‚   â”œâ”€â”€ processed/        # Datos procesados
â”‚   â””â”€â”€ external/         # Datos de fuentes externas
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_extraccion.py
â”‚   â”œâ”€â”€ 02_limpieza.py
â”‚   â”œâ”€â”€ 03_transformacion.py
â”‚   â””â”€â”€ 04_analisis.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploracion.qmd
â”‚   â””â”€â”€ validacion.qmd
â”‚
â”œâ”€â”€ src/                  # CÃ³digo reutilizable
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ database.py
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ tables/
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ metodologia.md
â”‚
â”œâ”€â”€ tests/                # Tests unitarios
â”‚   â””â”€â”€ test_utils.py
â”‚
â”œâ”€â”€ .env                  # Variables de ambiente (NO subir a git)
â”œâ”€â”€ .env.example          # Template de .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## 4.3 Ejemplo Real

Ver este repositorio para un ejemplo completo:

ğŸ”— [https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto](https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto)

**Nota importante:** No existe una estructura "perfecta". Adapta segÃºn:

- TamaÃ±o del proyecto

- NÃºmero de colaboradores

- TecnologÃ­as utilizadas

- Requisitos de deployment

## 4.4 Cookiecutter: Templates de Proyectos

Cookiecutter permite crear proyectos desde templates predefinidos.

### InstalaciÃ³n

```{bash}
#| eval: false
pip install cookiecutter
```

### Usar un Template

```{bash}
#| eval: false

# Template para Data Science
cookiecutter https://github.com/drivendata/cookiecutter-data-science

# Responder las preguntas interactivas:
# project_name: Mi Proyecto Analytics
# repo_name: mi_proyecto_analytics
# author_name: Tu Nombre
# etc.
```

### Crear Tu Propio Template

```{bash}
#| eval: false

# Estructura bÃ¡sica de un template
cookiecutter-mi-template/
â”œâ”€â”€ cookiecutter.json          # ConfiguraciÃ³n
â””â”€â”€ {{cookiecutter.repo_name}}/
    â”œâ”€â”€ data/
    â”œâ”€â”€ scripts/
    â”œâ”€â”€ src/
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

Archivo `cookiecutter.json`:

```{python}
#| eval: false

{
    "project_name": "Mi Proyecto",
    "repo_name": "{{ cookiecutter.project_name.lower().replace(' ', '_') }}",
    "author_name": "Nombre del Autor",
    "python_version": "3.11"
}
```

---

# 5. Pipelines y OrquestaciÃ³n

## 5.1 Â¿Por QuÃ© Modularizar?

Imagina un anÃ¡lisis tÃ­pico:

1. Extraer datos de BD
2. Limpiar datos
3. Construir features
4. Entrenar modelo
5. Generar reportes

### âŒ Enfoque MonolÃ­tico

```{python}
#| eval: false

# Un solo script gigante de 1000+ lÃ­neas
# DifÃ­cil de mantener, debuggear y reutilizar
import pandas as pd

# ... 100 lÃ­neas de extracciÃ³n ...
# ... 200 lÃ­neas de limpieza ...
# ... 300 lÃ­neas de feature engineering ...
# ... 400 lÃ­neas de modelado ...
```

### âœ… Enfoque Modular

```{bash}
#| eval: false

# Scripts separados, cada uno con responsabilidad Ãºnica
scripts/
â”œâ”€â”€ 01_extraer_datos.py       # Solo extracciÃ³n
â”œâ”€â”€ 02_limpiar_datos.py       # Solo limpieza
â”œâ”€â”€ 03_crear_features.py      # Solo feature engineering
â”œâ”€â”€ 04_entrenar_modelo.py     # Solo modelado
â””â”€â”€ 05_generar_reporte.py     # Solo reporting
```

**Ventajas:**

- ğŸ” **Debuggeable**: fÃ¡cil identificar dÃ³nde fallÃ³

- ğŸ”„ **Reutilizable**: usar solo los pasos necesarios

- ğŸ¤ **Colaborativo**: diferentes personas trabajan en diferentes mÃ³dulos

- âš¡ **Eficiente**: re-ejecutar solo pasos modificados

---

## 5.2 OrquestaciÃ³n con Bash

Un script Bash puede ejecutar todos los pasos secuencialmente.

### Crear Pipeline Bash

Archivo `run_pipeline.sh`:

```{bash}
#| eval: false
#| filename: "scripts/run_pipeline.sh"

#!/bin/bash

# Pipeline de procesamiento de datos
# Ejecuta scripts en orden secuencial

echo "ğŸš€ Iniciando pipeline..."
echo "================================"

# ConfiguraciÃ³n
PYTHON=python
SCRIPTS_DIR=scripts
YEAR=2024

# Paso 1: Extraer datos
echo ""
echo "ğŸ“¥ PASO 1: Extrayendo datos..."
$PYTHON $SCRIPTS_DIR/01_extraer_datos.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "âŒ Error en extracciÃ³n. Pipeline detenido."
    exit 1
fi
echo "âœ… ExtracciÃ³n completada"

# Paso 2: Limpiar datos
echo ""
echo "ğŸ§¹ PASO 2: Limpiando datos..."
$PYTHON $SCRIPTS_DIR/02_limpiar_datos.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "âŒ Error en limpieza. Pipeline detenido."
    exit 1
fi
echo "âœ… Limpieza completada"

# Paso 3: Crear features
echo ""
echo "âš™ï¸  PASO 3: Creando features..."
$PYTHON $SCRIPTS_DIR/03_crear_features.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "âŒ Error en feature engineering. Pipeline detenido."
    exit 1
fi
echo "âœ… Features creadas"

# Paso 4: Generar reporte
echo ""
echo "ğŸ“Š PASO 4: Generando reporte..."
$PYTHON $SCRIPTS_DIR/04_generar_reporte.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "âŒ Error en generaciÃ³n de reporte. Pipeline detenido."
    exit 1
fi
echo "âœ… Reporte generado"

echo ""
echo "================================"
echo "ğŸ‰ Pipeline completado exitosamente!"
```

### Ejecutar Pipeline

```{bash}
#| eval: false

# Dar permisos de ejecuciÃ³n
chmod +x scripts/run_pipeline.sh

# Ejecutar
./scripts/run_pipeline.sh
```

### Pipeline con Logging

```{bash}
#| eval: false
#| filename: "scripts/run_pipeline_logging.sh"

#!/bin/bash

# Pipeline con logging detallado

LOG_DIR=logs
mkdir -p $LOG_DIR
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE=$LOG_DIR/pipeline_$TIMESTAMP.log

echo "Iniciando pipeline - Log: $LOG_FILE"

# FunciÃ³n para logging
log_step() {
    echo "[$(date +%H:%M:%S)] $1" | tee -a $LOG_FILE
}

log_step "ğŸš€ Pipeline iniciado"

# Ejecutar pasos con logging
log_step "ğŸ“¥ Extrayendo datos..."
python scripts/01_extraer_datos.py --year 2024 2>&1 | tee -a $LOG_FILE
RESULT=${PIPESTATUS[0]}

if [ $RESULT -ne 0 ]; then
    log_step "âŒ Pipeline fallÃ³ en extracciÃ³n"
    exit 1
fi

log_step "âœ… Pipeline completado"
```

---

## 5.3 OrquestaciÃ³n con Python

Alternativamente, usar Python para orquestar:

```{python}
#| eval: false
#| filename: "scripts/run_pipeline.py"

#!/usr/bin/env python3
"""
Orquestador de pipeline en Python
"""

import os
import sys
from datetime import datetime

def run_step(step_name, script_path, args=None):
    """Ejecuta un paso del pipeline."""
    print(f"\n{'='*50}")
    print(f"ğŸ”„ Ejecutando: {step_name}")
    print(f"{'='*50}")
    
    # Construir comando
    cmd = f"python {script_path}"
    if args:
        cmd += " " + " ".join(args)
    
    print(f"Comando: {cmd}")
    
    # Ejecutar
    exit_code = os.system(cmd)
    
    if exit_code != 0:
        print(f"\nâŒ Error en {step_name}")
        print(f"CÃ³digo de salida: {exit_code}")
        sys.exit(1)
    
    print(f"âœ… {step_name} completado")
    return exit_code

def main():
    """Ejecuta pipeline completo."""
    start_time = datetime.now()
    
    print("ğŸš€ Iniciando pipeline...")
    print(f"Hora de inicio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ParÃ¡metros comunes
    year = "2024"
    
    # Definir pasos del pipeline
    steps = [
        {
            'name': 'ExtracciÃ³n de datos',
            'script': 'scripts/01_extraer_datos.py',
            'args': ['--year', year]
        },
        {
            'name': 'Limpieza de datos',
            'script': 'scripts/02_limpiar_datos.py',
            'args': ['--year', year]
        },
        {
            'name': 'Feature Engineering',
            'script': 'scripts/03_crear_features.py',
            'args': ['--year', year]
        },
        {
            'name': 'GeneraciÃ³n de reporte',
            'script': 'scripts/04_generar_reporte.py',
            'args': ['--year', year]
        }
    ]
    
    # Ejecutar cada paso
    for step in steps:
        run_step(step['name'], step['script'], step['args'])
    
    # Resumen final
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "="*50)
    print("ğŸ‰ Pipeline completado exitosamente!")
    print(f"DuraciÃ³n total: {duration}")
    print("="*50)

if __name__ == '__main__':
    main()
```

### Uso

```{bash}
#| eval: false

# Ejecutar orquestador
python scripts/run_pipeline.py
```

---

## 5.4 Conceptos de DAG

**DAG** = Directed Acyclic Graph (Grafo AcÃ­clico Dirigido)

### Â¿QuÃ© es un DAG?

Un DAG representa dependencias entre tareas:
- **Nodos**: tareas individuales
- **Aristas**: dependencias (quÃ© debe ejecutarse primero)
- **AcÃ­clico**: no hay loops (A depende de B, B de C, C de A âŒ)

### Ejemplo Visual

```
      [Extraer DB1]     [Extraer DB2]
            |                 |
            v                 v
        [Limpiar DB1]    [Limpiar DB2]
            |                 |
            +--------+--------+
                     |
                     v
              [Unir Datasets]
                     |
                     v
              [Feature Engineering]
                     |
                     v
            [Entrenar Modelo]
                     |
                     v
            [Generar Reporte]
```

### Ventajas de Pensar en DAGs

1. **ParalelizaciÃ³n**: tareas sin dependencias se ejecutan en paralelo
2. **RecuperaciÃ³n de errores**: re-ejecutar solo desde el punto de falla
3. **Claridad**: visualizar fÃ¡cilmente el flujo
4. **OptimizaciÃ³n**: identificar cuellos de botella

### DAG Simple en Python

```{python}
#| eval: false

# RepresentaciÃ³n simple de un DAG
pipeline_dag = {
    'extraer_db1': [],                          # Sin dependencias
    'extraer_db2': [],                          # Sin dependencias
    'limpiar_db1': ['extraer_db1'],            # Depende de extraer_db1
    'limpiar_db2': ['extraer_db2'],            # Depende de extraer_db2
    'unir_datasets': ['limpiar_db1', 'limpiar_db2'],  # Depende de ambos
    'feature_engineering': ['unir_datasets'],
    'entrenar_modelo': ['feature_engineering'],
    'generar_reporte': ['entrenar_modelo']
}

def ejecutar_dag(dag, funciones):
    """Ejecuta tareas respetando dependencias."""
    ejecutadas = set()
    
    def puede_ejecutar(tarea):
        """Verifica si todas las dependencias estÃ¡n satisfechas."""
        return all(dep in ejecutadas for dep in dag[tarea])
    
    while len(ejecutadas) < len(dag):
        for tarea, deps in dag.items():
            if tarea not in ejecutadas and puede_ejecutar(tarea):
                print(f"â–¶ï¸  Ejecutando: {tarea}")
                funciones[tarea]()  # Ejecutar funciÃ³n
                ejecutadas.add(tarea)
                print(f"âœ… Completado: {tarea}\n")
```

---

## 5.5 Herramientas Profesionales: Airflow

**Apache Airflow** es la herramienta lÃ­der para orquestaciÃ³n de pipelines complejos.

### CaracterÃ­sticas Principales

- ğŸ“… **Scheduling**: ejecutar pipelines en horarios especÃ­ficos
- ğŸ”„ **Retries**: reintentos automÃ¡ticos en caso de falla
- ğŸ“Š **Monitoring**: UI para visualizar estado de tareas
- ğŸ”€ **Dependencias complejas**: DAGs sofisticados
- ğŸ”Œ **Integraciones**: conectores para AWS, GCP, bases de datos, etc.

### Â¿CuÃ¡ndo Usar Airflow?

- âœ… Pipelines complejos con muchas dependencias
- âœ… Necesidad de scheduling robusto
- âœ… MÃºltiples fuentes de datos
- âœ… Equipos grandes que necesitan monitoreo centralizado
- âŒ Proyectos pequeÃ±os (overhead innecesario)
- âŒ Pipelines que se ejecutan manualmente

### Recursos para Aprender

- ğŸ“š DocumentaciÃ³n oficial: [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
- ğŸ“ Tutorial: [https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- ğŸ“¹ Episodio 140 de en_coders: [E140 - Airflow: Utilidad, elementos bÃ¡sicos y setup](https://www.youtube.com/watch?v=hh9P3dMOKwI&pp=ygURZW5fY29kZXJzIGFpcmZsb3c%3D)
- ğŸ“¹ YouTube: Buscar "Airflow tutorial for beginners"

**Nota**: Airflow tiene una curva de aprendizaje considerable. Empieza con scripts Bash/Python y migra a Airflow cuando lo necesites.

---

# 6. Ejemplo Completo: Pipeline End-to-End

Pongamos todo junto en un ejemplo funcional.

## 6.1 Estructura del Proyecto

```
proyecto_ejemplo/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ results/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_extraer_datos.py
â”‚   â”œâ”€â”€ 02_limpiar_datos.py
â”‚   â”œâ”€â”€ 03_crear_features.py
â”‚   â”œâ”€â”€ 04_generar_reporte.py
â”‚   â”œâ”€â”€ run_pipeline.sh
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â””â”€â”€ requirements.txt
```

## 6.2 Scripts del Pipeline

Los scripts completos estÃ¡n en los artifacts complementarios. AquÃ­ un resumen de cada uno:

### Script 1: ExtracciÃ³n de Datos

Extrae datos de fuente (simulado con dataset de sklearn)

### Script 2: Limpieza de Datos

Maneja valores nulos, outliers, y validaciones.

### Script 3: Feature Engineering

Crea variables derivadas para anÃ¡lisis.

### Script 4: GeneraciÃ³n de Reporte

Crea visualizaciones y mÃ©tricas resumidas.

---

# 7. Buenas PrÃ¡cticas - Resumen

## âœ… ParametrizaciÃ³n

1. **Definir parÃ¡metros al inicio** del script
2. **Usar nombres descriptivos** en MAYÃšSCULAS para constantes
3. **Variables sensibles en .env**, nunca hardcodeadas
4. **argparse para scripts CLI** que necesitan flexibilidad
5. **Validar parÃ¡metros** antes de usarlos
6. Usar rutas relativas en lugar de absolutas. Si requieres usar rutas absolutas por algÃºn motivo, nunca las hardcodees en el cÃ³digo. Ãšsalas como variables de ambiente o pÃ¡salas como argumentos.

## âœ… Estructura de Carpetas

1. **Separar datos crudos de procesados**
2. **Scripts numerados** para indicar orden de ejecuciÃ³n
3. **CÃ³digo reutilizable en `src/`**
4. **Documentar en README** la estructura del proyecto
5. **Usar .gitignore** para excluir datos/credenciales

## âœ… Pipelines

1. **Un script, una responsabilidad**
2. **Manejo de errores** en cada paso
3. **Logging detallado** para debugging
4. **Idempotencia**: mismo input = mismo output
5. **Tests para validar** cada mÃ³dulo

## âœ… Queries SQL

1. **Evitar SELECT *** excepto en exploraciÃ³n
2. **Parametrizar con precauciÃ³n** (SQL injection)
3. **Comentar queries complejas**
4. **Usar CTEs** para legibilidad
5. **Ãndices en columnas** filtradas frecuentemente

---

# 8. Ejercicios Propuestos

## Ejercicio 1: Refactorizar CÃ³digo

Dado este cÃ³digo malo, refactorÃ­zalo aplicando lo aprendido:

```{python}
#| eval: false

import pandas as pd

df = pd.read_csv('/Users/juan/Desktop/ventas.csv')
df = df[df['monto'] > 1000]
df = df[df['fecha'] >= '2024-01-01']
df.to_csv('/Users/juan/Desktop/resultados.csv')
print("Listo")
```

## Ejercicio 2: Crear Pipeline

Crea un pipeline de 3 pasos:
1. Leer CSV
2. Filtrar por condiciÃ³n
3. Guardar resultado

Usa argparse y orquesta con Bash.

## Ejercicio 3: Estructura de Carpetas

DiseÃ±a la estructura de carpetas para un proyecto que:
- Extrae datos de 3 fuentes diferentes
- Tiene notebooks de exploraciÃ³n (notebooks de Quarto, no de Jupyer Notebook, ya que estos Ãºltimos causan muchos problemas de versionamiento y otros inconvenientes)
- Genera reportes diarios
- Tiene tests unitarios

## Ejercicio 4: Reporte Parametrizado

- Crea un .qmd que reciba parÃ¡metros y genere reportes personalizados segÃºn lo que el usuario necesite

---

# 9. Recursos Adicionales

## DocumentaciÃ³n

- ğŸ“˜ **argparse**: [https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html)
- ğŸ“˜ **python-dotenv**: [https://pypi.org/project/python-dotenv/](https://pypi.org/project/python-dotenv/)
- ğŸ“˜ **Polars**: [https://pola.rs/](https://pola.rs/)
- ğŸ“˜ **Cookiecutter**: [https://cookiecutter.readthedocs.io/](https://cookiecutter.readthedocs.io/)
- ğŸ“˜ **Airflow**: [https://airflow.apache.org/](https://airflow.apache.org/)

## Libros Recomendados

- ğŸ“š "The Pragmatic Programmer" - Andy Hunt & Dave Thomas
- ğŸ“š "Clean Code" - Robert C. Martin (aplicable a Python)

## Repositorios de Ejemplo

- ğŸ”— [Cookiecutter Data Science](https://github.com/drivendata/cookiecutter-data-science)
- ğŸ”— [Ejemplo de modularizaciÃ³n](https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto)

---

# 10. Conclusiones

En esta clase aprendimos a:

1. âœ… **Parametrizar cÃ³digo** para hacerlo flexible y mantenible
2. âœ… **Usar .env** para gestionar configuraciones sensibles
3. âœ… **Crear scripts CLI** con argparse
4. âœ… **Parametrizar queries SQL** de forma segura
5. âœ… **Organizar proyectos** con estructura de carpetas clara
6. âœ… **Modularizar cÃ³digo** en scripts especializados
7. âœ… **Orquestar pipelines** con Bash y Python
8. âœ… **Entender DAGs** y su rol en pipelines complejos

## PrÃ³ximos Pasos

- ğŸ¯ Aplicar parametrizaciÃ³n en tus proyectos actuales
- ğŸ¯ Crear tu template de proyecto con cookiecutter
- ğŸ¯ Experimentar con pipelines simples
- ğŸ¯ Investigar Airflow cuando tengas pipelines complejos

---

**Â¡Gracias por tu atenciÃ³n!**

Â¿Preguntas?