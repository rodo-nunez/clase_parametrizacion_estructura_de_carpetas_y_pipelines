---
title: "Parametrizaci√≥n, Estructura de Carpetas y Pipelines"
subtitle: "De c√≥digo enredado a pipelines profesionales"
author: "Claude guiado y corregido por Rodo"
format:
  html:
    theme: darkly
    toc: true
    toc-depth: 3
    code-fold: false
    code-tools: true
    highlight-style: monokai
    self-contained: true
---

# 1. Introducci√≥n y Motivaci√≥n

## ¬øPor qu√© parametrizar?

Cuando trabajamos en proyectos de datos, es com√∫n empezar con c√≥digo "r√°pido y sucio" para probar ideas. Sin embargo, este c√≥digo presenta problemas:

- **Valores hardcodeados** dispersos por todo el script
- **Dif√≠cil mantenimiento**: cambiar un par√°metro requiere editar m√∫ltiples l√≠neas
- **No reutilizable**: cada nuevo caso requiere copiar y modificar el c√≥digo
- **Propenso a errores**: f√°cil olvidar actualizar alg√∫n valor
- **Dif√≠cil de versionar**: credenciales y configuraciones mezcladas con l√≥gica

**Objetivo de esta clase**: Transformar c√≥digo desorganizado en pipelines profesionales, mantenibles y escalables.

---

# 2. Parametrizaci√≥n de Variables

## 2.1 De C√≥digo Enredado a C√≥digo Limpio

Veamos un ejemplo t√≠pico de c√≥digo problem√°tico:

```{python}
#| eval: false
#| code-line-numbers: true

# ‚ùå C√ìDIGO MALO: valores hardcodeados y repetidos
import pandas as pd

# Cargar datos con ruta hardcodeada
df = pd.read_csv('/home/usuario/proyectos/datos/ventas_2024.csv')

# Filtrar por fecha (valor repetido)
df_filtrado = df[df['fecha'] >= '2024-01-01']
df_filtrado = df_filtrado[df_filtrado['fecha'] <= '2024-12-31']

# Calcular m√©tricas con umbral hardcodeado
clientes_importantes = df_filtrado[df_filtrado['total'] > 1000]

# Guardar en ruta hardcodeada
clientes_importantes.to_csv('/home/usuario/proyectos/resultados/clientes_2024.csv')

print(f"Procesados {len(df)} registros de 2024")
print(f"Encontrados {len(clientes_importantes)} clientes con compras > 1000")
```

**Problemas identificados:**

1. Rutas absolutas que solo funcionan en una m√°quina

2. Fechas repetidas (¬øqu√© pasa si queremos analizar 2025?)

3. Umbral m√°gico (1000) sin contexto

4. A√±o repetido en strings

Ahora refactoricemos paso a paso:

```{python}
#| eval: false

# ‚úÖ C√ìDIGO LIMPIO: parametrizado y mantenible
import pandas as pd
from pathlib import Path

# Definir par√°metros en un solo lugar
YEAR = 2024
UMBRAL_CLIENTE_IMPORTANTE = 1000
RUTA_DATOS = Path('data')
RUTA_RESULTADOS = Path('results')

# Crear rutas si no existen
RUTA_RESULTADOS.mkdir(exist_ok=True)

# Nombres de archivos parametrizados
archivo_entrada = RUTA_DATOS / f'ventas_{YEAR}.csv'
archivo_salida = RUTA_RESULTADOS / f'clientes_{YEAR}.csv'

# Cargar datos
df = pd.read_csv(archivo_entrada)

# Filtrar por fecha usando el par√°metro
fecha_inicio = f'{YEAR}-01-01'
fecha_fin = f'{YEAR}-12-31'
df_filtrado = df[
    (df['fecha'] >= fecha_inicio) & 
    (df['fecha'] <= fecha_fin)
]

# Calcular m√©tricas usando par√°metro descriptivo
clientes_importantes = df_filtrado[
    df_filtrado['total'] > UMBRAL_CLIENTE_IMPORTANTE
]

# Guardar resultados
clientes_importantes.to_csv(archivo_salida, index=False)

# Reportar resultados
print(f"Procesados {len(df)} registros de {YEAR}")
print(f"Encontrados {len(clientes_importantes)} clientes con compras > ${UMBRAL_CLIENTE_IMPORTANTE}")
```

**Mejoras logradas:**

- ‚úÖ Par√°metros definidos una sola vez

- ‚úÖ Nombres descriptivos en MAY√öSCULAS para constantes

- ‚úÖ Rutas relativas usando `pathlib`. No es necesario usar `pathlib`, es una opci√≥n. Pero lo realmente importante es no hardcodear rutas absolutas, sino que usar rutas relativas al proyecto (no al archivo)

- ‚úÖ F√°cil modificar para otro a√±o o umbral

---

## 2.2 Variables de Ambiente con .env

Para informaci√≥n sensible (credenciales, API keys) o configuraciones que cambian entre entornos (desarrollo, producci√≥n), usamos archivos `.env`.

### Instalaci√≥n

```{bash}
#| eval: false
pip install python-dotenv
```

### Crear archivo .env

Crear archivo `.env` en la ra√≠z del proyecto:

```{bash}
#| eval: false
#| filename: ".env"

# Configuraci√≥n de base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=analytics_db
DB_USER=analyst
DB_PASSWORD=mi_password_secreto

# Configuraci√≥n de APIs
API_KEY=abc123xyz789
API_ENDPOINT=https://api.ejemplo.com

# Configuraci√≥n del proyecto
ENVIRONMENT=development
LOG_LEVEL=INFO
```

### Uso en Python

```{python}
#| eval: false

import os
from dotenv import load_dotenv

# Cargar variables de ambiente
load_dotenv()

# Acceder a las variables
db_host = os.getenv('DB_HOST')
db_port = os.getenv('DB_PORT')
db_name = os.getenv('DB_NAME')
api_key = os.getenv('API_KEY')

# Valores por defecto si no existe la variable
log_level = os.getenv('LOG_LEVEL', 'WARNING')

print(f"Conectando a {db_host}:{db_port}/{db_name}")
print(f"Nivel de log: {log_level}")
```

### üîí Buenas Pr√°cticas de Seguridad

1. **NUNCA** subir `.env` a Git:

```{bash}
#| eval: false
#| filename: ".gitignore"

# Ignorar archivos de ambiente
.env
.env.local
.env.*.local
```

2. Crear `.env.example` como plantilla:

```{bash}
#| eval: false
#| filename: ".env.example"

# Configuraci√≥n de base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=nombre_base_datos
DB_USER=tu_usuario
DB_PASSWORD=tu_password

# Configuraci√≥n de APIs
API_KEY=tu_api_key_aqui
```

3. Documentar en README c√≥mo configurar el `.env`


4. A veces es bueno tener m√∫ltiples archivos `.env` para diferentes entornos:

- `.env.development`
- `.env.production`
- `.env.testing`
- `.env`
- etc.

Por defecto `python-dotenv` carga `.env`, pero puedes especificar otro archivo si es necesario. 

Quiz√°s algunos de ellos s√≠ pueden comitearse, y otros no. Por defecto, los archivos `.env` no deber√≠an comitearse nunca.

---

## 2.3 Argumentos de L√≠nea de Comandos con argparse

`argparse` permite ejecutar scripts con diferentes par√°metros sin modificar el c√≥digo.

### Ejemplo B√°sico

Crear archivo `procesar_ventas.py`:

```{python}
#| eval: false
#| filename: "scripts/procesar_ventas.py"

import argparse
import pandas as pd
from pathlib import Path

def procesar_ventas(year, umbral, verbose=False):
    """Procesa ventas de un a√±o espec√≠fico."""
    
    # Rutas
    archivo_entrada = Path('data') / f'ventas_{year}.csv'
    archivo_salida = Path('results') / f'clientes_{year}.csv'
    
    if verbose:
        print(f"üìÇ Leyendo {archivo_entrada}...")
    
    # Aqu√≠ ir√≠a la l√≥gica real de procesamiento
    # Para el ejemplo, creamos datos dummy
    df = pd.DataFrame({
        'cliente_id': range(1, 101),
        'total': [500 + i * 15 for i in range(100)]
    })
    
    clientes_importantes = df[df['total'] > umbral]
    
    # Guardar
    Path('results').mkdir(exist_ok=True)
    clientes_importantes.to_csv(archivo_salida, index=False)
    
    if verbose:
        print(f"‚úÖ Procesados {len(df)} registros")
        print(f"‚úÖ {len(clientes_importantes)} clientes > ${umbral}")
        print(f"üíæ Guardado en {archivo_salida}")
    
    return len(clientes_importantes)

def main():
    # Configurar parser
    parser = argparse.ArgumentParser(
        description='Procesa ventas y filtra clientes importantes',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Argumentos requeridos
    parser.add_argument(
        '--year',
        type=int,
        required=True,
        help='A√±o a procesar (ej: 2024)'
    )
    
    # Argumentos opcionales con valores por defecto
    parser.add_argument(
        '--umbral',
        type=float,
        default=1000,
        help='Umbral m√≠nimo para cliente importante'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Mostrar mensajes detallados'
    )
    
    # Parsear argumentos
    args = parser.parse_args()
    
    # Ejecutar procesamiento
    n_clientes = procesar_ventas(
        year=args.year,
        umbral=args.umbral,
        verbose=args.verbose
    )
    
    print(f"Proceso completado: {n_clientes} clientes importantes")

if __name__ == '__main__':
    main()
```

### Uso desde Terminal

```{bash}
#| eval: false

# Ver ayuda
python scripts/procesar_ventas.py --help

# Ejecuci√≥n b√°sica (solo argumento requerido)
python scripts/procesar_ventas.py --year 2024

# Con todos los par√°metros
python scripts/procesar_ventas.py --year 2024 --umbral 1500 --verbose

# Forma corta para verbose
python scripts/procesar_ventas.py --year 2025 -v
```

### Tipos de Argumentos Comunes

```{python}
#| eval: false

# String
parser.add_argument('--nombre', type=str, required=True)

# Integer
parser.add_argument('--num-registros', type=int, default=100)

# Float
parser.add_argument('--tasa', type=float, default=0.05)

# Boolean flag
parser.add_argument('--debug', action='store_true')

# Lista de valores
parser.add_argument('--columnas', nargs='+', help='Lista de columnas')

# Opciones limitadas
parser.add_argument(
    '--formato', 
    choices=['csv', 'parquet', 'excel'],
    default='csv'
)
```

---

# 3. Parametrizaci√≥n de Queries

## 3.1 SQL con f-strings

Al construir queries din√°micamente, necesitamos insertar par√°metros. Los f-strings son √∫tiles pero debemos usarlos con cuidado.

### Ejemplo Seguro

```{python}
#| eval: false

import pandas as pd
import sqlite3

# Crear base de datos de ejemplo
conn = sqlite3.connect(':memory:')

# Crear tabla de ejemplo
ventas_data = pd.DataFrame({
    'fecha': pd.date_range('2024-01-01', periods=100),
    'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Laptop'],
    'cantidad': range(1, 101),
    'precio': [1000, 25, 75] * 33 + [1000]
})
ventas_data.to_sql('ventas', conn, index=False, if_exists='replace')

# Par√°metros
fecha_inicio = '2024-01-01'
fecha_fin = '2024-03-31'
producto_filtro = 'Laptop'
cantidad_minima = 50

# Query parametrizada con f-strings
query = f"""
SELECT 
    fecha,
    producto,
    cantidad,
    precio,
    cantidad * precio as total
FROM ventas
WHERE fecha BETWEEN '{fecha_inicio}' AND '{fecha_fin}'
    AND producto = '{producto_filtro}'
    AND cantidad >= {cantidad_minima}
ORDER BY fecha DESC
"""

resultado = pd.read_sql(query, conn)
print(resultado.head())

conn.close()
```

### ‚ö†Ô∏è Prevenci√≥n de SQL Injection

**NUNCA** usar f-strings con input de usuarios sin validaci√≥n:

```{python}
#| eval: false

# ‚ùå PELIGROSO: vulnerable a SQL injection
producto_usuario = input("¬øQu√© producto buscar? ")
query = f"SELECT * FROM ventas WHERE producto = '{producto_usuario}'"

# ‚úÖ SEGURO: usar par√°metros con placeholder
query = "SELECT * FROM ventas WHERE producto = ?"
resultado = pd.read_sql(query, conn, params=(producto_usuario,))
```

### Construcci√≥n Din√°mica de Queries

```{python}
#| eval: false

def construir_query_ventas(
    columnas=None,
    fecha_inicio=None,
    fecha_fin=None,
    productos=None,
    limite=None
):
    """Construye query din√°micamente seg√∫n par√°metros."""
    
    # Columnas por defecto
    if columnas is None:
        columnas = ['fecha', 'producto', 'cantidad', 'precio']
    
    select_clause = ', '.join(columnas)
    
    # Construir WHERE din√°micamente
    condiciones = []
    
    if fecha_inicio:
        condiciones.append(f"fecha >= '{fecha_inicio}'")
    
    if fecha_fin:
        condiciones.append(f"fecha <= '{fecha_fin}'")
    
    if productos:
        productos_str = "', '".join(productos)
        condiciones.append(f"producto IN ('{productos_str}')")
    
    where_clause = ' AND '.join(condiciones) if condiciones else '1=1'
    
    # Construir query completa
    query = f"""
    SELECT {select_clause}
    FROM ventas
    WHERE {where_clause}
    """
    
    if limite:
        query += f"\nLIMIT {limite}"
    
    return query

# Ejemplo de uso
query1 = construir_query_ventas(
    fecha_inicio='2024-01-01',
    productos=['Laptop', 'Mouse'],
    limite=10
)
print(query1)
```

---

## 3.2 Parametrizaci√≥n con Polars (Opcional)

Polars es una alternativa moderna y r√°pida a pandas. Ofrece formas elegantes de parametrizar queries.

```{python}
#| eval: false

import polars as pl

# Crear DataFrame de ejemplo
df = pl.DataFrame({
    'fecha': pl.date_range(
        start=pl.date(2024, 1, 1),
        end=pl.date(2024, 12, 31),
        interval='1d'
    ),
    'producto': ['Laptop', 'Mouse', 'Teclado'] * 122,
    'cantidad': range(1, 367),
    'precio': [1000, 25, 75] * 122
})

# Par√°metros
FECHA_INICIO = pl.date(2024, 6, 1)
FECHA_FIN = pl.date(2024, 6, 30)
PRODUCTOS_INTERES = ['Laptop', 'Mouse']
CANTIDAD_MIN = 100

# Query parametrizada con Polars
resultado = (
    df
    .filter(
        (pl.col('fecha') >= FECHA_INICIO) &
        (pl.col('fecha') <= FECHA_FIN) &
        (pl.col('producto').is_in(PRODUCTOS_INTERES)) &
        (pl.col('cantidad') >= CANTIDAD_MIN)
    )
    .with_columns(
        (pl.col('cantidad') * pl.col('precio')).alias('total')
    )
    .sort('fecha', descending=True)
)

print(resultado)
```

**Ventajas de Polars:**

- M√°s r√°pido que pandas en datasets grandes

- Sintaxis expresiva y type-safe

- Lazy evaluation para optimizaci√≥n autom√°tica

---

# 4. Estructura de Carpetas

## 4.1 Principios de Organizaci√≥n

Una buena estructura de carpetas:

- **Facilita encontrar archivos**: ubicaci√≥n predecible
- **Separa concerns**: datos, c√≥digo, resultados, documentaci√≥n
- **Escalable**: crece con el proyecto sin volverse ca√≥tica
- **Reproducible**: otros pueden entender y ejecutar el proyecto

## 4.2 Estructura Recomendada

```
proyecto_analytics/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Datos originales (NUNCA modificar)
‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Datos procesados
‚îÇ   ‚îî‚îÄ‚îÄ external/         # Datos de fuentes externas
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_extraccion.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_limpieza.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_transformacion.py
‚îÇ   ‚îî‚îÄ‚îÄ 04_analisis.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ exploracion.qmd
‚îÇ   ‚îî‚îÄ‚îÄ validacion.qmd
‚îÇ
‚îú‚îÄ‚îÄ src/                  # C√≥digo reutilizable
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ utils.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îî‚îÄ‚îÄ database.py
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îî‚îÄ‚îÄ tables/
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ metodologia.md
‚îÇ
‚îú‚îÄ‚îÄ tests/                # Tests unitarios
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îÇ
‚îú‚îÄ‚îÄ .env                  # Variables de ambiente (NO subir a git)
‚îú‚îÄ‚îÄ .env.example          # Template de .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## 4.3 Ejemplo Real

Ver este repositorio para un ejemplo completo:

üîó [https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto](https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto)

**Nota importante:** No existe una estructura "perfecta". Adapta seg√∫n:

- Tama√±o del proyecto

- N√∫mero de colaboradores

- Tecnolog√≠as utilizadas

- Requisitos de deployment

## 4.4 Cookiecutter: Templates de Proyectos

Cookiecutter permite crear proyectos desde templates predefinidos.

### Instalaci√≥n

```{bash}
#| eval: false
pip install cookiecutter
```

### Usar un Template

```{bash}
#| eval: false

# Template para Data Science
cookiecutter https://github.com/drivendata/cookiecutter-data-science

# Responder las preguntas interactivas:
# project_name: Mi Proyecto Analytics
# repo_name: mi_proyecto_analytics
# author_name: Tu Nombre
# etc.
```

### Crear Tu Propio Template

```{bash}
#| eval: false

# Estructura b√°sica de un template
cookiecutter-mi-template/
‚îú‚îÄ‚îÄ cookiecutter.json          # Configuraci√≥n
‚îî‚îÄ‚îÄ {{cookiecutter.repo_name}}/
    ‚îú‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ README.md
    ‚îî‚îÄ‚îÄ requirements.txt
```

Archivo `cookiecutter.json`:

```{python}
#| eval: false

{
    "project_name": "Mi Proyecto",
    "repo_name": "{{ cookiecutter.project_name.lower().replace(' ', '_') }}",
    "author_name": "Nombre del Autor",
    "python_version": "3.11"
}
```

---

# 5. Pipelines y Orquestaci√≥n

## 5.1 ¬øPor Qu√© Modularizar?

Imagina un an√°lisis t√≠pico:

1. Extraer datos de BD
2. Limpiar datos
3. Construir features
4. Entrenar modelo
5. Generar reportes

### ‚ùå Enfoque Monol√≠tico

```{python}
#| eval: false

# Un solo script gigante de 1000+ l√≠neas
# Dif√≠cil de mantener, debuggear y reutilizar
import pandas as pd

# ... 100 l√≠neas de extracci√≥n ...
# ... 200 l√≠neas de limpieza ...
# ... 300 l√≠neas de feature engineering ...
# ... 400 l√≠neas de modelado ...
```

### ‚úÖ Enfoque Modular

```{bash}
#| eval: false

# Scripts separados, cada uno con responsabilidad √∫nica
scripts/
‚îú‚îÄ‚îÄ 01_extraer_datos.py       # Solo extracci√≥n
‚îú‚îÄ‚îÄ 02_limpiar_datos.py       # Solo limpieza
‚îú‚îÄ‚îÄ 03_crear_features.py      # Solo feature engineering
‚îú‚îÄ‚îÄ 04_entrenar_modelo.py     # Solo modelado
‚îî‚îÄ‚îÄ 05_generar_reporte.py     # Solo reporting
```

**Ventajas:**

- üîç **Debuggeable**: f√°cil identificar d√≥nde fall√≥

- üîÑ **Reutilizable**: usar solo los pasos necesarios

- ü§ù **Colaborativo**: diferentes personas trabajan en diferentes m√≥dulos

- ‚ö° **Eficiente**: re-ejecutar solo pasos modificados

---

## 5.2 Orquestaci√≥n con Bash

Un script Bash puede ejecutar todos los pasos secuencialmente.

### Crear Pipeline Bash

Archivo `run_pipeline.sh`:

```{bash}
#| eval: false
#| filename: "scripts/run_pipeline.sh"

#!/bin/bash

# Pipeline de procesamiento de datos
# Ejecuta scripts en orden secuencial

echo "üöÄ Iniciando pipeline..."
echo "================================"

# Configuraci√≥n
PYTHON=python
SCRIPTS_DIR=scripts
YEAR=2024

# Paso 1: Extraer datos
echo ""
echo "üì• PASO 1: Extrayendo datos..."
$PYTHON $SCRIPTS_DIR/01_extraer_datos.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "‚ùå Error en extracci√≥n. Pipeline detenido."
    exit 1
fi
echo "‚úÖ Extracci√≥n completada"

# Paso 2: Limpiar datos
echo ""
echo "üßπ PASO 2: Limpiando datos..."
$PYTHON $SCRIPTS_DIR/02_limpiar_datos.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "‚ùå Error en limpieza. Pipeline detenido."
    exit 1
fi
echo "‚úÖ Limpieza completada"

# Paso 3: Crear features
echo ""
echo "‚öôÔ∏è  PASO 3: Creando features..."
$PYTHON $SCRIPTS_DIR/03_crear_features.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "‚ùå Error en feature engineering. Pipeline detenido."
    exit 1
fi
echo "‚úÖ Features creadas"

# Paso 4: Generar reporte
echo ""
echo "üìä PASO 4: Generando reporte..."
$PYTHON $SCRIPTS_DIR/04_generar_reporte.py --year $YEAR
if [ $? -ne 0 ]; then
    echo "‚ùå Error en generaci√≥n de reporte. Pipeline detenido."
    exit 1
fi
echo "‚úÖ Reporte generado"

echo ""
echo "================================"
echo "üéâ Pipeline completado exitosamente!"
```

### Ejecutar Pipeline

```{bash}
#| eval: false

# Dar permisos de ejecuci√≥n
chmod +x scripts/run_pipeline.sh

# Ejecutar
./scripts/run_pipeline.sh
```

### Pipeline con Logging

```{bash}
#| eval: false
#| filename: "scripts/run_pipeline_logging.sh"

#!/bin/bash

# Pipeline con logging detallado

LOG_DIR=logs
mkdir -p $LOG_DIR
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE=$LOG_DIR/pipeline_$TIMESTAMP.log

echo "Iniciando pipeline - Log: $LOG_FILE"

# Funci√≥n para logging
log_step() {
    echo "[$(date +%H:%M:%S)] $1" | tee -a $LOG_FILE
}

log_step "üöÄ Pipeline iniciado"

# Ejecutar pasos con logging
log_step "üì• Extrayendo datos..."
python scripts/01_extraer_datos.py --year 2024 2>&1 | tee -a $LOG_FILE
RESULT=${PIPESTATUS[0]}

if [ $RESULT -ne 0 ]; then
    log_step "‚ùå Pipeline fall√≥ en extracci√≥n"
    exit 1
fi

log_step "‚úÖ Pipeline completado"
```

---

## 5.3 Orquestaci√≥n con Python

Alternativamente, usar Python para orquestar:

```{python}
#| eval: false
#| filename: "scripts/run_pipeline.py"

#!/usr/bin/env python3
"""
Orquestador de pipeline en Python
"""

import os
import sys
from datetime import datetime

def run_step(step_name, script_path, args=None):
    """Ejecuta un paso del pipeline."""
    print(f"\n{'='*50}")
    print(f"üîÑ Ejecutando: {step_name}")
    print(f"{'='*50}")
    
    # Construir comando
    cmd = f"python {script_path}"
    if args:
        cmd += " " + " ".join(args)
    
    print(f"Comando: {cmd}")
    
    # Ejecutar
    exit_code = os.system(cmd)
    
    if exit_code != 0:
        print(f"\n‚ùå Error en {step_name}")
        print(f"C√≥digo de salida: {exit_code}")
        sys.exit(1)
    
    print(f"‚úÖ {step_name} completado")
    return exit_code

def main():
    """Ejecuta pipeline completo."""
    start_time = datetime.now()
    
    print("üöÄ Iniciando pipeline...")
    print(f"Hora de inicio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Par√°metros comunes
    year = "2024"
    
    # Definir pasos del pipeline
    steps = [
        {
            'name': 'Extracci√≥n de datos',
            'script': 'scripts/01_extraer_datos.py',
            'args': ['--year', year]
        },
        {
            'name': 'Limpieza de datos',
            'script': 'scripts/02_limpiar_datos.py',
            'args': ['--year', year]
        },
        {
            'name': 'Feature Engineering',
            'script': 'scripts/03_crear_features.py',
            'args': ['--year', year]
        },
        {
            'name': 'Generaci√≥n de reporte',
            'script': 'scripts/04_generar_reporte.py',
            'args': ['--year', year]
        }
    ]
    
    # Ejecutar cada paso
    for step in steps:
        run_step(step['name'], step['script'], step['args'])
    
    # Resumen final
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "="*50)
    print("üéâ Pipeline completado exitosamente!")
    print(f"Duraci√≥n total: {duration}")
    print("="*50)

if __name__ == '__main__':
    main()
```

### Uso

```{bash}
#| eval: false

# Ejecutar orquestador
python scripts/run_pipeline.py
```

---

## 5.4 Conceptos de DAG

**DAG** = Directed Acyclic Graph (Grafo Ac√≠clico Dirigido)

### ¬øQu√© es un DAG?

Un DAG representa dependencias entre tareas:
- **Nodos**: tareas individuales
- **Aristas**: dependencias (qu√© debe ejecutarse primero)
- **Ac√≠clico**: no hay loops (A depende de B, B de C, C de A ‚ùå)

### Ejemplo Visual

```
      [Extraer DB1]     [Extraer DB2]
            |                 |
            v                 v
        [Limpiar DB1]    [Limpiar DB2]
            |                 |
            +--------+--------+
                     |
                     v
              [Unir Datasets]
                     |
                     v
              [Feature Engineering]
                     |
                     v
            [Entrenar Modelo]
                     |
                     v
            [Generar Reporte]
```

### Ventajas de Pensar en DAGs

1. **Paralelizaci√≥n**: tareas sin dependencias se ejecutan en paralelo
2. **Recuperaci√≥n de errores**: re-ejecutar solo desde el punto de falla
3. **Claridad**: visualizar f√°cilmente el flujo
4. **Optimizaci√≥n**: identificar cuellos de botella

### DAG Simple en Python

```{python}
#| eval: false

# Representaci√≥n simple de un DAG
pipeline_dag = {
    'extraer_db1': [],                          # Sin dependencias
    'extraer_db2': [],                          # Sin dependencias
    'limpiar_db1': ['extraer_db1'],            # Depende de extraer_db1
    'limpiar_db2': ['extraer_db2'],            # Depende de extraer_db2
    'unir_datasets': ['limpiar_db1', 'limpiar_db2'],  # Depende de ambos
    'feature_engineering': ['unir_datasets'],
    'entrenar_modelo': ['feature_engineering'],
    'generar_reporte': ['entrenar_modelo']
}

def ejecutar_dag(dag, funciones):
    """Ejecuta tareas respetando dependencias."""
    ejecutadas = set()
    
    def puede_ejecutar(tarea):
        """Verifica si todas las dependencias est√°n satisfechas."""
        return all(dep in ejecutadas for dep in dag[tarea])
    
    while len(ejecutadas) < len(dag):
        for tarea, deps in dag.items():
            if tarea not in ejecutadas and puede_ejecutar(tarea):
                print(f"‚ñ∂Ô∏è  Ejecutando: {tarea}")
                funciones[tarea]()  # Ejecutar funci√≥n
                ejecutadas.add(tarea)
                print(f"‚úÖ Completado: {tarea}\n")
```

---

## 5.5 Herramientas Profesionales: Airflow

**Apache Airflow** es la herramienta l√≠der para orquestaci√≥n de pipelines complejos.

### Caracter√≠sticas Principales

- üìÖ **Scheduling**: ejecutar pipelines en horarios espec√≠ficos
- üîÑ **Retries**: reintentos autom√°ticos en caso de falla
- üìä **Monitoring**: UI para visualizar estado de tareas
- üîÄ **Dependencias complejas**: DAGs sofisticados
- üîå **Integraciones**: conectores para AWS, GCP, bases de datos, etc.

### ¬øCu√°ndo Usar Airflow?

- ‚úÖ Pipelines complejos con muchas dependencias
- ‚úÖ Necesidad de scheduling robusto
- ‚úÖ M√∫ltiples fuentes de datos
- ‚úÖ Equipos grandes que necesitan monitoreo centralizado
- ‚ùå Proyectos peque√±os (overhead innecesario)
- ‚ùå Pipelines que se ejecutan manualmente

### Recursos para Aprender

- üìö Documentaci√≥n oficial: [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
- üéì Tutorial: [https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- üìπ Episodio 140 de en_coders: [E140 - Airflow: Utilidad, elementos b√°sicos y setup](https://www.youtube.com/watch?v=hh9P3dMOKwI&pp=ygURZW5fY29kZXJzIGFpcmZsb3c%3D)
- üìπ YouTube: Buscar "Airflow tutorial for beginners"

**Nota**: Airflow tiene una curva de aprendizaje considerable. Empieza con scripts Bash/Python y migra a Airflow cuando lo necesites.

---

# 6. Ejemplo Completo: Pipeline End-to-End

Pongamos todo junto en un ejemplo funcional.

## 6.1 Estructura del Proyecto

```
proyecto_ejemplo/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îú‚îÄ‚îÄ results/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_extraer_datos.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_limpiar_datos.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_crear_features.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_generar_reporte.py
‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline.sh
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ requirements.txt
```

## 6.2 Scripts del Pipeline

Los scripts completos est√°n en los artifacts complementarios. Aqu√≠ un resumen de cada uno:

### Script 1: Extracci√≥n de Datos

Extrae datos de fuente (simulado con dataset de sklearn)

### Script 2: Limpieza de Datos

Maneja valores nulos, outliers, y validaciones.

### Script 3: Feature Engineering

Crea variables derivadas para an√°lisis.

### Script 4: Generaci√≥n de Reporte

Crea visualizaciones y m√©tricas resumidas.

---

# 7. Buenas Pr√°cticas - Resumen

## ‚úÖ Parametrizaci√≥n

1. **Definir par√°metros al inicio** del script
2. **Usar nombres descriptivos** en MAY√öSCULAS para constantes
3. **Variables sensibles en .env**, nunca hardcodeadas
4. **argparse para scripts CLI** que necesitan flexibilidad
5. **Validar par√°metros** antes de usarlos
6. Usar rutas relativas en lugar de absolutas. Si requieres usar rutas absolutas por alg√∫n motivo, nunca las hardcodees en el c√≥digo. √ösalas como variables de ambiente o p√°salas como argumentos.

## ‚úÖ Estructura de Carpetas

1. **Separar datos crudos de procesados**
2. **Scripts numerados** para indicar orden de ejecuci√≥n
3. **C√≥digo reutilizable en `src/`**
4. **Documentar en README** la estructura del proyecto
5. **Usar .gitignore** para excluir datos/credenciales

## ‚úÖ Pipelines

1. **Un script, una responsabilidad**
2. **Manejo de errores** en cada paso
3. **Logging detallado** para debugging
4. **Idempotencia**: mismo input = mismo output
5. **Tests para validar** cada m√≥dulo

## ‚úÖ Queries SQL

1. **Evitar SELECT *** excepto en exploraci√≥n
2. **Parametrizar con precauci√≥n** (SQL injection)
3. **Comentar queries complejas**
4. **Usar CTEs** para legibilidad
5. **√çndices en columnas** filtradas frecuentemente

---

# 8. Ejercicios Propuestos

## Ejercicio 1: Refactorizar C√≥digo

Dado este c√≥digo malo, refactor√≠zalo aplicando lo aprendido:

```{python}
#| eval: false

import pandas as pd

df = pd.read_csv('/Users/juan/Desktop/ventas.csv')
df = df[df['monto'] > 1000]
df = df[df['fecha'] >= '2024-01-01']
df.to_csv('/Users/juan/Desktop/resultados.csv')
print("Listo")
```

## Ejercicio 2: Crear Pipeline

Crea un pipeline de 3 pasos:
1. Leer CSV
2. Filtrar por condici√≥n
3. Guardar resultado

Usa argparse y orquesta con Bash.

## Ejercicio 3: Estructura de Carpetas

Dise√±a la estructura de carpetas para un proyecto que:
- Extrae datos de 3 fuentes diferentes
- Tiene notebooks de exploraci√≥n (notebooks de Quarto, no de Jupyer Notebook, ya que estos √∫ltimos causan muchos problemas de versionamiento y otros inconvenientes)
- Genera reportes diarios
- Tiene tests unitarios

## Ejercicio 4: Reporte Parametrizado

- Crea un .qmd que reciba par√°metros y genere reportes personalizados seg√∫n lo que el usuario necesite

---

# 9. Recursos Adicionales

## Documentaci√≥n

- üìò **argparse**: [https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html)
- üìò **python-dotenv**: [https://pypi.org/project/python-dotenv/](https://pypi.org/project/python-dotenv/)
- üìò **Polars**: [https://pola.rs/](https://pola.rs/)
- üìò **Cookiecutter**: [https://cookiecutter.readthedocs.io/](https://cookiecutter.readthedocs.io/)
- üìò **Airflow**: [https://airflow.apache.org/](https://airflow.apache.org/)

## Libros Recomendados

- üìö "The Pragmatic Programmer" - Andy Hunt & Dave Thomas
- üìö "Clean Code" - Robert C. Martin (aplicable a Python)

## Repositorios de Ejemplo

- üîó [Cookiecutter Data Science](https://github.com/drivendata/cookiecutter-data-science)
- üîó [Ejemplo de modularizaci√≥n](https://github.com/rodo-nunez/ejemplo_de_modularizacion_de_proyecto)

---

# 10. Conclusiones

En esta clase aprendimos a:

1. ‚úÖ **Parametrizar c√≥digo** para hacerlo flexible y mantenible
2. ‚úÖ **Usar .env** para gestionar configuraciones sensibles
3. ‚úÖ **Crear scripts CLI** con argparse
4. ‚úÖ **Parametrizar queries SQL** de forma segura
5. ‚úÖ **Organizar proyectos** con estructura de carpetas clara
6. ‚úÖ **Modularizar c√≥digo** en scripts especializados
7. ‚úÖ **Orquestar pipelines** con Bash y Python
8. ‚úÖ **Entender DAGs** y su rol en pipelines complejos

## Pr√≥ximos Pasos

- üéØ Aplicar parametrizaci√≥n en tus proyectos actuales
- üéØ Crear tu template de proyecto con cookiecutter
- üéØ Experimentar con pipelines simples
- üéØ Investigar Airflow cuando tengas pipelines complejos

---

**¬°Gracias por tu atenci√≥n!**

¬øPreguntas?